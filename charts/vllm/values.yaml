nameOverride: ""
fullnameOverride: ""

deploy:
  strategy:
    type: Recreate

  replicaCount: 1

  annotations: {}
  podAnnotations: {}

  podSecurityContext: {}
  securityContext: {}
  
  imagePullSecrets: []
  image:
    repository: vllm/vllm-openai
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: "v0.5.0"
  
  # Customize the port the server API will listen on
  port: 8000
  
  # Override the resources if you need more
  resources:
    limits:
      nvidia.com/gpu: "1"
    requests:
      nvidia.com/gpu: "1"

  # Change this if you want to serve another model
  model: mistralai/Mistral-7B-Instruct-v0.3
  
  # Add nodeSelectors to target specific GPU types
  nodeSelector: {}
  
  tolerations: []

  affinity: {}
  
  # Extra arguments
  # Ref: https://docs.vllm.ai/en/stable/serving/openai_compatible_server.html#command-line-arguments-for-the-server
  args:
  - "--model"
  - "mistralai/Mistral-7B-Instruct-v0.3"
  - "--shm-size=16G"
  - "--download-dir"
  - "/model"


  # Override env variables
  # Ref: https://docs.vllm.ai/en/stable/serving/env_vars.html
  env: []
  
  
  volumes: []
  #   - name: model
  #     hostPath:
  #       path: /model

  volumeMounts: []
  #   - name: model
  #     mountPath: /model

  pvcCache:
    enabled: false
    annotations: {}
    accessModes:
    - ReadWriteOnce
    size:  50Gi
    storageClass: oci-bv
    cachePath: /root/.cache


service:
  name: ""
  type: ClusterIP
  port: 80

ingress:
  enabled: false
  className: "nginx"
  annotations: {}
    # kubernetes.io/ingress.class: nginx
    # kubernetes.io/tls-acme: "true"
  hosts:
  - example.com
  pathPrefix: "/"
  pathType: Prefix
  tls: []
  #  - secretName: example-tls
  #    hosts:
  #      - example.com
